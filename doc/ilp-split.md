NOTE: THIS FILE WAS GENERATED BY AN LLM FOR A PREVIOUS TRAIN-TEST-VALIDATION
PROBLEM. IT MAY CONTAIN INACCURACIES OR OTHERWISE CONFUSING INFORMATION.

The train-validation split in the main repository is based on this documentation
and modified to work for PHQ-9 scores ranging from 0--27.

### **Train/Test Split Optimization Using Integer Linear Programming**

To ensure an effective and unbiased evaluation of our quality prediction model, we implemented an Integer Linear Programming (ILP) approach to determine an optimal train/test split of the dataset. This methodology was chosen to satisfy multiple objectives: maintaining an approximate 80/20 ratio of training to testing samples, ensuring balanced representation across the quality score labels (MOS values ranging from 1 to 5) in the test set, and preventing overfitting by enforcing a database-level partitioning. Below, we elucidate the rationale, formulation, and operational mechanics of the ILP approach employed in our study.

#### **Rationale for Using Integer Linear Programming**

The task of splitting datasets into training and testing subsets is a fundamental step in machine learning workflows. However, achieving an optimal balance between maintaining representative label distributions and adhering to predefined split ratios can be challenging, especially when data is organized at hierarchical levels, such as databases containing multiple files. Traditional random sampling methods may inadvertently lead to imbalanced label distributions or violate the desired split ratios, potentially compromising the model's generalizability and evaluation integrity.

Integer Linear Programming offers a robust framework to address this complexity by formulating the split as an optimization problem with clearly defined objectives and constraints. Specifically, ILP enables:

1. **Precise Control Over Split Ratios:** By defining constraints that enforce the desired proportion of training and testing samples, ensuring adherence to the 80/20 split.

2. **Balanced Label Representation:** By optimizing the allocation of databases to the test set such that the distribution of quality score labels closely mirrors the overall dataset, thereby facilitating a comprehensive evaluation across all label categories.

3. **Prevention of Data Leakage:** By enforcing database-level partitioning, ILP ensures that entire databases are assigned to either the training or testing set. This strategy mitigates the risk of the model learning database-specific nuances that do not generalize to unseen data, thereby enhancing the model's robustness.

#### **Formulation of the Integer Linear Programming Model**

The ILP model was constructed with the following components:

1. **Decision Variables:**
   - **Binary Variables (xᵢ):** For each database *i*, a binary variable *xᵢ* was defined, where *xᵢ* = 1 if database *i* is assigned to the test set, and *xᵢ* = 0 otherwise.

2. **Objective Function:**
   - The primary objective was to **minimize the total deviation** between the actual and desired distributions of quality score labels in the test set. Mathematically, this was represented as:

     \[
     \text{Minimize} \quad \sum_{l=1}^{5} d_{l}
     \]

     where *dₗ* denotes the deviation for label *l* from its target count in the test set.

3. **Constraints:**
   - **Train/Test Split Ratio:**
     - To maintain an approximate 80/20 split, constraints were set to ensure that the total number of files in the test set falls within a specified tolerance range around 20% of the entire dataset. Formally,

       \[
       0.15 \times T \leq \sum_{i} (f_{i} \times x_{i}) \leq 0.25 \times T
       \]

       where *T* is the total number of files, and *fᵢ* represents the number of files in database *i*.

   - **Balanced Label Distribution:**
     - For each quality score label *l*, constraints were established to limit the deviation from the desired count *Dₗ* (where *Dₗ* = 20% of *T* / 5 for balanced representation):

       \[
       \sum_{i} (c_{i,l} \times x_{i}) - D_{l} \leq d_{l}
       \]
       \[
       D_{l} - \sum_{i} (c_{i,l} \times x_{i}) \leq d_{l}
       \]

       Here, *cᵢₗ* denotes the number of files with label *l* in database *i*. These constraints ensure that the deviation *dₗ* captures the absolute difference between the actual and desired label counts in the test set.

   - **Binary Assignment:**
     - Each database must be exclusively assigned to either the training or testing set, reinforced by the binary nature of *xᵢ*.

4. **Solver Implementation:**
   - The formulated ILP model was implemented using Python's PuLP library, interfacing with the CBC (Coin-or branch and cut) solver. The solver was tasked with identifying the optimal set of binary variables *xᵢ* that minimize the total label deviation while satisfying the split ratio and balanced distribution constraints.

#### **Operational Mechanics of the ILP Approach**

The ILP solver operates by systematically exploring feasible assignments of databases to the test set, guided by the defined constraints and objective function. The process entails:

1. **Initialization:**
   - The solver initializes by evaluating all possible combinations of database assignments that satisfy the 80/20 split constraints.

2. **Optimization:**
   - Among the feasible assignments, the solver seeks the configuration that results in the smallest cumulative deviation from the desired label distributions in the test set. This involves linearizing the absolute deviations using auxiliary variables to maintain the problem's linearity.

3. **Solution Extraction:**
   - Upon identifying an optimal or near-optimal solution, the solver outputs the set of databases designated for the test set, ensuring that the constraints are respected.

4. **Validation:**
   - The resultant split was validated by examining the actual distribution of labels within the test set, confirming adherence to the balanced representation goal and the approximate 80/20 split.

#### **Advantages of the ILP Approach in This Context**

The application of ILP in determining the train/test split offers several compelling advantages:

- **Precision and Flexibility:** ILP allows for the precise specification of multiple, possibly conflicting, constraints, enabling a balanced consideration of both split ratios and label distributions.

- **Optimality:** The solver ensures that the chosen split is not only feasible but also optimal with respect to the defined objective, minimizing the likelihood of biased or unrepresentative test sets.

- **Scalability:** While computationally demanding for exceptionally large datasets, ILP remains effective for moderately sized datasets with a manageable number of databases, as in our case.

- **Reproducibility:** The deterministic nature of ILP ensures that the split is reproducible, a critical factor in experimental research for validation and comparison purposes.

#### **Conclusion**

Employing Integer Linear Programming provided a structured and mathematically grounded method to achieve a nuanced train/test split that balances multiple objectives: maintaining an 80/20 ratio, ensuring balanced label representations, and enforcing database-level partitioning to prevent overfitting. This methodological rigor underpins the reliability and validity of our DNN model's evaluation, contributing to the overall robustness of our research findings.
